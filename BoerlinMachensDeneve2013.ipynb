{
 "metadata": {
  "name": "BoerlinMachensDeneve2013"
 },
 "nbformat": 3,
 "nbformat_minor": 0,
 "worksheets": [
  {
   "cells": [
    {
     "cell_type": "heading",
     "level": 1,
     "metadata": {},
     "source": [
      "Predictive Coding of Dynamical Variables in Balanced Spiking Networks, Martin Boerlin, Christian K. Machens, Sophie Den\u00e8ve, Nov, 2013"
     ]
    },
    {
     "cell_type": "heading",
     "level": 2,
     "metadata": {},
     "source": [
      "Formalism"
     ]
    },
    {
     "cell_type": "heading",
     "level": 3,
     "metadata": {},
     "source": [
      "Setup"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "$J$ variables of dynamical system:\n",
      "\n",
      "$\\dot{\\mathbf{x}}(t)=\\mathbf{A}\\mathbf{x}(t)+\\mathbf{c}(t)$ ...Eqn(1)\n",
      "\n",
      "are encoded in the delta-function spike trains $\\mathbf{o}$(t) of $N$ leaky integrate-and-fire neurons,\n",
      "\n",
      "which are read out [by another layer?]:\n",
      "\n",
      "$\\dot{\\mathbf{\\hat{x}}}=-\\lambda_d \\mathbf{\\hat{x}} + \\mathbf{\\Gamma} \\mathbf{o}(t)$ ...Eqn(2).\n",
      "\n",
      "The integration of each spike in train $i$ contributes a decaying exponential $\\exp(-\\lambda_d\\tau)$ weighted by $\\Gamma_{ij}$ to the read-out variable $\\hat{x}_j$."
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "$\\hat{\\mathbf{x}}=\\frac{1}{\\lambda_d}\\mathbf{\\Gamma}\\mathbf{r}(t)$, where \n",
      "\n",
      "$\\dot{\\mathbf{r}}(t) = -\\lambda_d \\mathbf{r}(t) + \\lambda_d \\mathbf{o}(t)$ ...Eqn(3)"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "*[* Check: Insert $\\mathbf{r}(t) = \\lambda_d \\mathbf{\\Gamma}^{-1}\\hat{\\mathbf{x}}$ into $\\dot{\\mathbf{r}}(t) = -\\lambda_d \\mathbf{r}(t) + \\lambda_d \\mathbf{o}(t)$:"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "$\\lambda_d \\mathbf{\\Gamma}^{-1}\\dot{\\mathbf{\\hat{x}}} = -\\lambda_d^2 \\mathbf{\\Gamma}^{-1}\\mathbf{\\hat{x}} + \\lambda_d \\mathbf{o}(t)$ same as Eqn(2)."
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "I think this assumes $\\mathbf{\\Gamma}$ is invertible, even if we manipulate Eqn(2) to get Eqn(3).*]*"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "*[* The readout seems to be rate-based? What if we cascade two such predictive coding systems? *]*"
     ]
    },
    {
     "cell_type": "heading",
     "level": 3,
     "metadata": {},
     "source": [
      "Optimization / learning"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "Assume network greedy-optimizes cost function ($||\\cdot||_1$ and $||\\cdot||_2$ refer to L1 and L2 norms):\n",
      "\n",
      "$E(t) = \\int_0^t du \\left(|| \\mathbf{x}(u)-\\mathbf{\\hat{x}}(u)||_2^2 +\\nu ||\\mathbf{r}(u)||_1 +\\mu ||\\mathbf{r}(u)||_2^2 \\right)~~$     ...Eqn(4)\n",
      "\n",
      "by varying spike-times, but $\\mathbf{\\Gamma}$ (matrix) is kept constant, unlike in liquid computing approaches."
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [],
     "language": "python",
     "metadata": {},
     "outputs": []
    }
   ],
   "metadata": {}
  }
 ]
}